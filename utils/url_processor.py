from typing import List, Optional
from urllib.parse import urljoin, urlparse, parse_qs
import xml.etree.ElementTree as ET
import requests
import logging
import re

logger = logging.getLogger(__name__)

class URLProcessor:
    """Handles URL processing, validation, and sitemap parsing."""

    def __init__(self, domain: str, base_paths: List[str], headers: dict, timeout: int):
        self.domain = domain
        self.base_paths = base_paths
        self.headers = headers
        self.timeout = timeout

    def is_relevant_url(self, url: str, language: str) -> bool:
        """Check if URL is relevant based on domain, path, and language."""
        logger.debug(f"ğŸ” Checking URL relevance: {url}")
        
        parsed_url = urlparse(url)
        logger.debug(f"  Domain check: {parsed_url.netloc} vs {self.domain}")
        
        if parsed_url.netloc != self.domain:
            logger.debug(f"  âŒ Domain mismatch: {parsed_url.netloc} != {self.domain}")
            return False

        logger.debug(f"  Path check: {parsed_url.path} against base paths: {self.base_paths}")
        is_relevant = False
        for base_path in self.base_paths:
            if parsed_url.path.startswith(base_path):
                logger.debug(f"  âœ… Path matches base path: {base_path}")
                is_relevant = True
                break

        if not is_relevant:
            logger.debug(f"  âŒ Path doesn't match any base path")
            return False

        query_params = parse_qs(parsed_url.query)
        url_language = query_params.get('hl', [None])[0]
        logger.debug(f"  Language check: URL language={url_language}, target={language}")

        # Enhanced language detection for path-based routing
        if language == 'en':
            # For English, accept both no language parameter and explicit en
            if url_language is None:
                # Check for path-based language routing
                if f'/{language}/' in parsed_url.path or parsed_url.path.startswith(f'/{language}'):
                    logger.debug(f"  âœ… English path-based routing detected")
                    return True
                elif any(lang in parsed_url.path for lang in ['/fr/', '/de/', '/es/', '/pt/', '/ja/', '/ko/', '/zh/']):
                    logger.debug(f"  âŒ Non-English path-based routing detected")
                    return False
                else:
                    logger.debug(f"  âœ… Default English (no language indicator)")
                    return True
            else:
                result = url_language == 'en'
                logger.debug(f"  English query parameter result: {result}")
                return result

        # For non-English languages, check both query params and path
        if url_language == language:
            logger.debug(f"  âœ… Query parameter language match")
            return True
        elif f'/{language}/' in parsed_url.path or parsed_url.path.startswith(f'/{language}'):
            logger.debug(f"  âœ… Path-based language match")
            return True
        
        logger.debug(f"  âŒ No language match found")
        return False

    def find_sitemap_url(self, base_url: str) -> Optional[str]:
        """Try to find sitemap URL from robots.txt or common locations."""
        logger.info(f"ğŸ” Starting sitemap discovery for: {base_url}")
        
        try:
            robots_url = urljoin(base_url, '/robots.txt')
            logger.info(f"ğŸ“„ Checking robots.txt at {robots_url}")
            response = requests.get(robots_url, headers=self.headers, timeout=self.timeout)
            logger.info(f"âœ… robots.txt response: {response.status_code}")
            
            if response.status_code == 200:
                logger.info(f"ğŸ“ robots.txt content preview: {response.text[:200]}...")
                sitemap_match = re.search(r'Sitemap: (.*)', response.text)
                if sitemap_match:
                    sitemap_url = sitemap_match.group(1).strip()
                    logger.info(f"ğŸ¯ Found sitemap in robots.txt: {sitemap_url}")
                    return sitemap_url
                else:
                    logger.info("ğŸ“„ No sitemap directive found in robots.txt")

            # Try common sitemap locations
            logger.info("ğŸ” Trying common sitemap locations...")
            common_paths = ['/sitemap.xml', '/sitemap_index.xml', '/sitemap/sitemap.xml']
            for path in common_paths:
                url = urljoin(base_url, path)
                logger.info(f"ğŸŒ Trying: {url}")
                try:
                    test_response = requests.get(url, headers=self.headers, timeout=self.timeout)
                    logger.info(f"ğŸ“¡ Response for {url}: {test_response.status_code}")
                    if test_response.status_code == 200:
                        logger.info(f"âœ… Found sitemap at: {url}")
                        return url
                except requests.RequestException as e:
                    logger.info(f"âŒ Failed to access {url}: {e}")
                    continue

        except requests.RequestException as e:
            logger.error(f"ğŸ’¥ Error during sitemap discovery: {e}")
        
        logger.warning(f"ğŸš« No XML sitemap found for {base_url}")
        logger.info(f"ğŸ”„ Falling back to HTML discovery method")
        return base_url  # Return base URL for HTML parsing

    def parse_sitemap(self, sitemap_url: str) -> List[str]:
        """Parse XML sitemap or HTML page and return list of URLs."""
        logger.info(f"ğŸ“Š Starting sitemap parsing for: {sitemap_url}")
        
        # Check if this is HTML fallback (base URL) or XML sitemap
        if sitemap_url.endswith('.xml'):
            return self._parse_xml_sitemap(sitemap_url)
        else:
            logger.info(f"ğŸ”„ Using HTML discovery method for: {sitemap_url}")
            return self.parse_html_sitemap(sitemap_url)
    
    def _parse_xml_sitemap(self, sitemap_url: str) -> List[str]:
        """Parse XML sitemap and return list of URLs."""
        try:
            logger.info(f"ğŸŒ Fetching XML sitemap content...")
            response = requests.get(sitemap_url, headers=self.headers, timeout=self.timeout)
            logger.info(f"ğŸ“¡ Sitemap response: {response.status_code}, Content-Type: {response.headers.get('content-type', 'unknown')}")
            logger.info(f"ğŸ“ Content length: {len(response.content)} bytes")
            
            # Log first 500 chars to see what we're actually getting
            content_preview = response.content.decode('utf-8', errors='ignore')[:500]
            logger.info(f"ğŸ“ Content preview: {content_preview}...")
            
            logger.info(f"ğŸ”¬ Attempting XML parsing...")
            root = ET.fromstring(response.content)
            logger.info(f"âœ… XML parsing successful, root tag: {root.tag}")
            
            namespaces = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
            loc_elements = root.findall('.//ns:loc', namespaces)
            logger.info(f"ğŸ” Found {len(loc_elements)} <loc> elements in sitemap")
            
            urls = [loc.text for loc in loc_elements if loc.text]
            logger.info(f"ğŸ“‹ Extracted {len(urls)} valid URLs from XML sitemap")
            
            # Log first few URLs for debugging
            for i, url in enumerate(urls[:5]):
                logger.info(f"  {i+1}. {url}")
            if len(urls) > 5:
                logger.info(f"  ... and {len(urls) - 5} more URLs")
            
            return urls

        except ET.ParseError as e:
            logger.error(f"âŒ XML parsing error for {sitemap_url}: {e}")
            logger.error(f"ğŸ”§ This might be an HTML page instead of XML sitemap")
            return []
        except Exception as e:
            logger.error(f"ğŸ’¥ Unexpected error parsing XML sitemap {sitemap_url}: {e}")
            return []

    def parse_html_sitemap(self, html_url: str, max_depth: int = 2) -> List[str]:
        """Parse HTML page to extract documentation links with recursive discovery."""
        logger.info(f"ğŸŒ TRACE: parse_html_sitemap() - Entry point with max_depth={max_depth}")
        logger.info(f"ğŸŒ Starting recursive HTML discovery for: {html_url}")
        logger.info(f"ğŸŒ TRACE: NEW FEATURE - Implementing recursive crawling up to {max_depth} levels!")
        
        discovered_urls = set()
        processed_urls = set()
        url_queue = [(html_url, 0)]  # (url, depth)
        
        while url_queue:
            current_url, current_depth = url_queue.pop(0)
            
            if current_url in processed_urls or current_depth >= max_depth:
                continue
                
            logger.info(f"ğŸ” Processing level {current_depth + 1}: {current_url}")
            processed_urls.add(current_url)
            
            level_urls = self._extract_links_from_page(current_url)
            
            for url in level_urls:
                if url not in discovered_urls and url not in processed_urls:
                    discovered_urls.add(url)
                    if current_depth + 1 < max_depth:
                        url_queue.append((url, current_depth + 1))
                        logger.debug(f"ğŸ“ Queued for level {current_depth + 2}: {url}")
        
        result_urls = list(discovered_urls)
        logger.info(f"ğŸ¯ Recursive discovery complete: {len(result_urls)} URLs found across {max_depth} levels")
        return result_urls
    
    def _extract_links_from_page(self, html_url: str) -> List[str]:
        """Extract documentation links from a single HTML page."""
        try:
            logger.debug(f"ğŸ“„ Fetching HTML content from: {html_url}")
            response = requests.get(html_url, headers=self.headers, timeout=self.timeout)
            logger.debug(f"ğŸ“¡ HTML response: {response.status_code}, Content-Type: {response.headers.get('content-type', 'unknown')}")
            
            if response.status_code != 200:
                logger.error(f"âŒ Failed to fetch HTML: {response.status_code}")
                return []
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            logger.debug(f"âœ… HTML parsing successful")
            
            # Extract all links from the page
            links = soup.find_all('a', href=True)
            logger.debug(f"ğŸ” Found {len(links)} total links in HTML")
            
            # Filter for relevant documentation links
            doc_urls = []
            for link in links:
                href = link.get('href')
                if href:
                    # Convert relative URLs to absolute
                    from urllib.parse import urljoin
                    absolute_url = urljoin(html_url, href)
                    
                    # Basic filtering for documentation-like URLs
                    if self.is_documentation_link(absolute_url, html_url):
                        doc_urls.append(absolute_url)
            
            # Remove duplicates
            unique_urls = list(set(doc_urls))
            logger.debug(f"ğŸ“‹ Extracted {len(unique_urls)} unique documentation URLs from current page")
            
            # Log first few URLs for debugging
            for i, url in enumerate(unique_urls[:3]):
                logger.debug(f"  {i+1}. {url}")
            if len(unique_urls) > 3:
                logger.debug(f"  ... and {len(unique_urls) - 3} more URLs")
            
            return unique_urls
            
        except Exception as e:
            logger.error(f"ğŸ’¥ Error extracting links from {html_url}: {e}")
            return []
    
    def is_documentation_link(self, url: str, base_url: str) -> bool:
        """Check if a link appears to be documentation content."""
        from urllib.parse import urlparse
        
        parsed_url = urlparse(url)
        parsed_base = urlparse(base_url)
        
        # Must be same domain
        if parsed_url.netloc != parsed_base.netloc:
            return False
        
        # Skip common non-documentation patterns
        skip_patterns = [
            '/login', '/signup', '/register', '/auth',
            '/api/', '/admin/', '/account/', '/profile/',
            'mailto:', 'tel:', 'javascript:',
            '.pdf', '.doc', '.zip', '.img', '.png', '.jpg', '.gif',
            '#', '?search=', '/search'
        ]
        
        url_lower = url.lower()
        for pattern in skip_patterns:
            if pattern in url_lower:
                return False
        
        # Look for documentation indicators
        doc_indicators = [
            '/article', '/articles', '/docs', '/help', '/guide', '/guides',
            '/documentation', '/tutorial', '/tutorials', '/kb', '/knowledge',
            '/support', '/faq', '/how-to', '/getting-started'
        ]
        
        for indicator in doc_indicators:
            if indicator in url_lower:
                return True
        
        # For Intercom-style help centers, check for article patterns
        if '/en/' in url and any(x in url_lower for x in ['article', 'collection']):
            return True
        
        # If path starts with base path and has content depth
        path_parts = parsed_url.path.strip('/').split('/')
        if len(path_parts) >= 2:  # Has some content depth
            for base_path in self.base_paths:
                if parsed_url.path.startswith(base_path.rstrip('/')):
                    return True
        
        return False