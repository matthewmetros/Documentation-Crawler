@app.route('/api/start-crawling', methods=['POST'])
def start_crawling():
    """Start a new crawling session."""
    try:
        logger.info("ğŸŒ BACKEND: /api/start-crawling endpoint called")
        
        data = request.get_json()
        if not data:
            logger.error("ğŸŒ BACKEND: No JSON data received")
            return jsonify({
                'success': False,
                'error': 'No data provided'
            }), 400
            
        # Validate required fields
        if not data.get('url'):
            logger.error("ğŸŒ BACKEND: No URL provided")
            return jsonify({
                'success': False,
                'error': 'URL is required'
            }), 400
            
        logger.info(f"ğŸŒ BACKEND: Received request data: {data}")
        
        # Validate format selection
        if not any([data.get('store_markdown'), data.get('store_raw_html'), data.get('store_text')]):
            logger.error("ğŸŒ BACKEND: No output format selected")
            return jsonify({
                'success': False,
                'error': 'At least one output format must be selected'
            }), 400
        
        session_id = str(uuid.uuid4())
        logger.info(f"ğŸŒ BACKEND: Generated session ID: {session_id}")
        
        # Create new crawler session
        crawler_interface = CrawlerWebInterface(session_id, socketio)
        
        with session_lock:
            active_sessions[session_id] = {
                'crawler': crawler_interface,
                'started_at': datetime.now().isoformat(),
                'config': data,
                'url': data.get('url', ''),
                'status': 'initializing',
                'progress': 0,
                'total_pages': 0
            }
        
        # Start crawling in background thread
        crawl_thread = threading.Thread(
            target=crawler_interface.start_crawling,
            args=(data,)
        )
        crawl_thread.daemon = True
        crawl_thread.start()
        
        return jsonify({
            'success': True,
            'session_id': session_id,
            'message': 'Crawling session started'
        })
        
    except Exception as e:
        logger.error(f"Error starting crawling: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500